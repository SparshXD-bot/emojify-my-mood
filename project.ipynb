{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bbc3500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    label\n",
      "0                            i didnt feel humiliated  sadness\n",
      "1  i can go from feeling so hopeless to so damned...  sadness\n",
      "2   im grabbing a minute to post i feel greedy wrong    anger\n",
      "3  i am ever feeling nostalgic about the fireplac...     love\n",
      "4                               i am feeling grouchy    anger\n",
      "label\n",
      "joy         5362\n",
      "sadness     4666\n",
      "anger       2159\n",
      "fear        1937\n",
      "love        1304\n",
      "surprise     572\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read the dataset (assuming train.txt has text + label separated by tab)\n",
    "df = pd.read_csv(\"C:/Users/spars/Downloads/train_emojify.txt\", sep=\";\", names=[\"text\", \"label\"])\n",
    "\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b724c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\spars\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\spars\\miniconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\spars\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2025.9.18-cp312-cp312-win_amd64.whl (275 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.3.0 nltk-3.9.2 regex-2025.9.18\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d85086de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\spars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacfb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove anything that's not a letter (keep spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # split into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    # join back into a sentence\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2d65d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                            i didnt feel humiliated   \n",
      "1  i can go from feeling so hopeless to so damned...   \n",
      "2   im grabbing a minute to post i feel greedy wrong   \n",
      "3  i am ever feeling nostalgic about the fireplac...   \n",
      "4                               i am feeling grouchy   \n",
      "5  ive been feeling a little burdened lately wasn...   \n",
      "6  ive been taking or milligrams or times recomme...   \n",
      "7  i feel as confused about life as a teenager or...   \n",
      "8  i have been with petronas for years i feel tha...   \n",
      "9                                i feel romantic too   \n",
      "\n",
      "                                          clean_text  \n",
      "0                              didnt feel humiliated  \n",
      "1  go feeling hopeless damned hopeful around some...  \n",
      "2          im grabbing minute post feel greedy wrong  \n",
      "3  ever feeling nostalgic fireplace know still pr...  \n",
      "4                                    feeling grouchy  \n",
      "5      ive feeling little burdened lately wasnt sure  \n",
      "6  ive taking milligrams times recommended amount...  \n",
      "7     feel confused life teenager jaded year old man  \n",
      "8  petronas years feel petronas performed well ma...  \n",
      "9                                      feel romantic  \n"
     ]
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(df[['text', 'clean_text']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0961b2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (16000, 5000)\n",
      "Example vector for 1st sentence:\n",
      " <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 3 stored elements and shape (1, 5000)>\n",
      "  Coords\tValues\n",
      "  (0, 1171)\t0.5951235084078971\n",
      "  (0, 1630)\t0.16379156905484632\n",
      "  (0, 2142)\t0.7867657412767967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF object\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # limit to 5000 features\n",
    "\n",
    "# Fit on training data and transform\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Labels (y values)\n",
    "y = df['label']\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Example vector for 1st sentence:\\n\", X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0b583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8621875\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.89      0.81      0.85       427\n",
      "        fear       0.85      0.77      0.81       397\n",
      "         joy       0.82      0.96      0.88      1021\n",
      "        love       0.89      0.62      0.74       296\n",
      "     sadness       0.90      0.94      0.92       946\n",
      "    surprise       0.88      0.47      0.61       113\n",
      "\n",
      "    accuracy                           0.86      3200\n",
      "   macro avg       0.87      0.76      0.80      3200\n",
      "weighted avg       0.87      0.86      0.86      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7820b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion to emoji mapping\n",
    "emoji_map = {\n",
    "    \"joy\": \"üòÑ\",\n",
    "    \"sadness\": \"üò¢\",\n",
    "    \"anger\": \"üò°\",\n",
    "    \"fear\": \"üò®\",\n",
    "    \"love\": \"‚ù§Ô∏è\",\n",
    "    \"surprise\": \"üò≤\"\n",
    "    \"cry\" \"üò≠\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e97e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: owais have a gf so he is happy  \n",
      "Predicted Emotion: joy\n",
      "Emoji: üòÑ\n"
     ]
    }
   ],
   "source": [
    "# Example custom input\n",
    "sample_text = [\"owais have a gf so he is happy  \"]\n",
    "\n",
    "# Transform text using the same vectorizer\n",
    "sample_vec = vectorizer.transform(sample_text)\n",
    "\n",
    "# Predict emotion\n",
    "pred_label = model.predict(sample_vec)[0]\n",
    "\n",
    "# Map to emoji\n",
    "print(\"Text:\", sample_text[0])\n",
    "print(\"Predicted Emotion:\", pred_label)\n",
    "print(\"Emoji:\", emoji_map[pred_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14edda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
